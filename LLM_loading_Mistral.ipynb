{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f9d119a8-5c71-4244-ba97-9e10463fd17e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: torch 2.1.2\n",
      "Uninstalling torch-2.1.2:\n",
      "  Successfully uninstalled torch-2.1.2\n",
      "Found existing installation: xformers 0.0.23\n",
      "Uninstalling xformers-0.0.23:\n",
      "  Successfully uninstalled xformers-0.0.23\n",
      "\u001b[33mWARNING: Skipping mistral_inference as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip uninstall -y torch xformers mistral_inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "26bebf03-6a27-4b42-9f54-e80d53eb7b63",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting torch==2.1.2\n",
      "  Downloading torch-2.1.2-cp311-cp311-manylinux1_x86_64.whl.metadata (25 kB)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from torch==2.1.2) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.11/site-packages (from torch==2.1.2) (4.12.2)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.11/site-packages (from torch==2.1.2) (1.13.1)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch==2.1.2) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch==2.1.2) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.11/site-packages (from torch==2.1.2) (2023.12.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch==2.1.2) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch==2.1.2) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch==2.1.2) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /opt/conda/lib/python3.11/site-packages (from torch==2.1.2) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/lib/python3.11/site-packages (from torch==2.1.2) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/lib/python3.11/site-packages (from torch==2.1.2) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/lib/python3.11/site-packages (from torch==2.1.2) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/lib/python3.11/site-packages (from torch==2.1.2) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/lib/python3.11/site-packages (from torch==2.1.2) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /opt/conda/lib/python3.11/site-packages (from torch==2.1.2) (2.18.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch==2.1.2) (12.1.105)\n",
      "Requirement already satisfied: triton==2.1.0 in /opt/conda/lib/python3.11/site-packages (from torch==2.1.2) (2.1.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.1.2) (12.4.127)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch==2.1.2) (2.1.4)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from sympy->torch==2.1.2) (1.3.0)\n",
      "Downloading torch-2.1.2-cp311-cp311-manylinux1_x86_64.whl (670.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m670.2/670.2 MB\u001b[0m \u001b[31m70.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: torch\n",
      "Successfully installed torch-2.1.2\n"
     ]
    }
   ],
   "source": [
    "!pip install torch==2.1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa1bc056-bb8b-42fb-9dfb-acf23efb2857",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting xformers==0.0.23\n",
      "  Downloading xformers-0.0.23-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.0 kB)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.11/site-packages (from xformers==0.0.23) (1.26.3)\n",
      "Collecting torch==2.1.1 (from xformers==0.0.23)\n",
      "  Downloading torch-2.1.1-cp311-cp311-manylinux1_x86_64.whl.metadata (25 kB)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from torch==2.1.1->xformers==0.0.23) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.11/site-packages (from torch==2.1.1->xformers==0.0.23) (4.12.2)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.11/site-packages (from torch==2.1.1->xformers==0.0.23) (1.13.1)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch==2.1.1->xformers==0.0.23) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch==2.1.1->xformers==0.0.23) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.11/site-packages (from torch==2.1.1->xformers==0.0.23) (2023.12.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch==2.1.1->xformers==0.0.23) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch==2.1.1->xformers==0.0.23) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch==2.1.1->xformers==0.0.23) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /opt/conda/lib/python3.11/site-packages (from torch==2.1.1->xformers==0.0.23) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/lib/python3.11/site-packages (from torch==2.1.1->xformers==0.0.23) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/lib/python3.11/site-packages (from torch==2.1.1->xformers==0.0.23) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/lib/python3.11/site-packages (from torch==2.1.1->xformers==0.0.23) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/lib/python3.11/site-packages (from torch==2.1.1->xformers==0.0.23) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/lib/python3.11/site-packages (from torch==2.1.1->xformers==0.0.23) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /opt/conda/lib/python3.11/site-packages (from torch==2.1.1->xformers==0.0.23) (2.18.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch==2.1.1->xformers==0.0.23) (12.1.105)\n",
      "Requirement already satisfied: triton==2.1.0 in /opt/conda/lib/python3.11/site-packages (from torch==2.1.1->xformers==0.0.23) (2.1.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.1.1->xformers==0.0.23) (12.4.127)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch==2.1.1->xformers==0.0.23) (2.1.4)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from sympy->torch==2.1.1->xformers==0.0.23) (1.3.0)\n",
      "Downloading xformers-0.0.23-cp311-cp311-manylinux2014_x86_64.whl (213.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m213.0/213.0 MB\u001b[0m \u001b[31m73.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading torch-2.1.1-cp311-cp311-manylinux1_x86_64.whl (670.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m670.2/670.2 MB\u001b[0m \u001b[31m67.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: torch, xformers\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.1.2\n",
      "    Uninstalling torch-2.1.2:\n",
      "      Successfully uninstalled torch-2.1.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchaudio 2.1.2 requires torch==2.1.2, but you have torch 2.1.1 which is incompatible.\n",
      "torchvision 0.16.2 requires torch==2.1.2, but you have torch 2.1.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed torch-2.1.1 xformers-0.0.23\n"
     ]
    }
   ],
   "source": [
    "!pip install xformers==0.0.23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dfa1a030-646b-40e4-9568-e4313d89134c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting mistral_inference\n",
      "  Downloading mistral_inference-1.5.0-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting fire>=0.6.0 (from mistral_inference)\n",
      "  Downloading fire-0.7.0.tar.gz (87 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting mistral_common>=1.4.0 (from mistral_inference)\n",
      "  Downloading mistral_common-1.5.1-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting pillow>=10.3.0 (from mistral_inference)\n",
      "  Downloading pillow-11.1.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (9.1 kB)\n",
      "Requirement already satisfied: safetensors>=0.4.0 in /opt/conda/lib/python3.11/site-packages (from mistral_inference) (0.4.5)\n",
      "Collecting simple-parsing>=0.1.5 (from mistral_inference)\n",
      "  Downloading simple_parsing-0.1.6-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting xformers>=0.0.24 (from mistral_inference)\n",
      "  Downloading xformers-0.0.29.post1-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (1.0 kB)\n",
      "Requirement already satisfied: termcolor in /opt/conda/lib/python3.11/site-packages (from fire>=0.6.0->mistral_inference) (2.4.0)\n",
      "Requirement already satisfied: jsonschema<5.0.0,>=4.21.1 in /opt/conda/lib/python3.11/site-packages (from mistral_common>=1.4.0->mistral_inference) (4.21.1)\n",
      "Requirement already satisfied: numpy>=1.25 in /opt/conda/lib/python3.11/site-packages (from mistral_common>=1.4.0->mistral_inference) (1.26.3)\n",
      "Collecting pillow>=10.3.0 (from mistral_inference)\n",
      "  Downloading pillow-10.4.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (9.2 kB)\n",
      "Collecting pydantic<3.0.0,>=2.6.1 (from mistral_common>=1.4.0->mistral_inference)\n",
      "  Downloading pydantic-2.10.4-py3-none-any.whl.metadata (29 kB)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /opt/conda/lib/python3.11/site-packages (from mistral_common>=1.4.0->mistral_inference) (2.31.0)\n",
      "Collecting sentencepiece==0.2.0 (from mistral_common>=1.4.0->mistral_inference)\n",
      "  Downloading sentencepiece-0.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Collecting tiktoken<0.8.0,>=0.7.0 (from mistral_common>=1.4.0->mistral_inference)\n",
      "  Downloading tiktoken-0.7.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting typing-extensions<5.0.0,>=4.11.0 (from mistral_common>=1.4.0->mistral_inference)\n",
      "  Downloading typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting docstring-parser<1.0,>=0.15 (from simple-parsing>=0.1.5->mistral_inference)\n",
      "  Downloading docstring_parser-0.16-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting torch==2.5.1 (from xformers>=0.0.24->mistral_inference)\n",
      "  Downloading torch-2.5.1-cp311-cp311-manylinux1_x86_64.whl.metadata (28 kB)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from torch==2.5.1->xformers>=0.0.24->mistral_inference) (3.13.1)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch==2.5.1->xformers>=0.0.24->mistral_inference) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch==2.5.1->xformers>=0.0.24->mistral_inference) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.11/site-packages (from torch==2.5.1->xformers>=0.0.24->mistral_inference) (2023.12.2)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch==2.5.1->xformers>=0.0.24->mistral_inference)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch==2.5.1->xformers>=0.0.24->mistral_inference)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch==2.5.1->xformers>=0.0.24->mistral_inference)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch==2.5.1->xformers>=0.0.24->mistral_inference)\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch==2.5.1->xformers>=0.0.24->mistral_inference)\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch==2.5.1->xformers>=0.0.24->mistral_inference)\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch==2.5.1->xformers>=0.0.24->mistral_inference)\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch==2.5.1->xformers>=0.0.24->mistral_inference)\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch==2.5.1->xformers>=0.0.24->mistral_inference)\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nccl-cu12==2.21.5 (from torch==2.5.1->xformers>=0.0.24->mistral_inference)\n",
      "  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.4.127 (from torch==2.5.1->xformers>=0.0.24->mistral_inference)\n",
      "  Downloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch==2.5.1->xformers>=0.0.24->mistral_inference)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting triton==3.1.0 (from torch==2.5.1->xformers>=0.0.24->mistral_inference)\n",
      "  Downloading triton-3.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\n",
      "Collecting sympy==1.13.1 (from torch==2.5.1->xformers>=0.0.24->mistral_inference)\n",
      "  Downloading sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from sympy==1.13.1->torch==2.5.1->xformers>=0.0.24->mistral_inference) (1.3.0)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /opt/conda/lib/python3.11/site-packages (from jsonschema<5.0.0,>=4.21.1->mistral_common>=1.4.0->mistral_inference) (23.2.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.11/site-packages (from jsonschema<5.0.0,>=4.21.1->mistral_common>=1.4.0->mistral_inference) (2023.12.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /opt/conda/lib/python3.11/site-packages (from jsonschema<5.0.0,>=4.21.1->mistral_common>=1.4.0->mistral_inference) (0.32.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.11/site-packages (from jsonschema<5.0.0,>=4.21.1->mistral_common>=1.4.0->mistral_inference) (0.17.1)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3.0.0,>=2.6.1->mistral_common>=1.4.0->mistral_inference)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.27.2 (from pydantic<3.0.0,>=2.6.1->mistral_common>=1.4.0->mistral_inference)\n",
      "  Downloading pydantic_core-2.27.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests<3.0.0,>=2.0.0->mistral_common>=1.4.0->mistral_inference) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests<3.0.0,>=2.0.0->mistral_common>=1.4.0->mistral_inference) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests<3.0.0,>=2.0.0->mistral_common>=1.4.0->mistral_inference) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests<3.0.0,>=2.0.0->mistral_common>=1.4.0->mistral_inference) (2023.11.17)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /opt/conda/lib/python3.11/site-packages (from tiktoken<0.8.0,>=0.7.0->mistral_common>=1.4.0->mistral_inference) (2024.11.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch==2.5.1->xformers>=0.0.24->mistral_inference) (2.1.4)\n",
      "Downloading mistral_inference-1.5.0-py3-none-any.whl (30 kB)\n",
      "Downloading mistral_common-1.5.1-py3-none-any.whl (6.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m97.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading sentencepiece-0.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m98.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pillow-10.4.0-cp311-cp311-manylinux_2_28_x86_64.whl (4.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m92.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading simple_parsing-0.1.6-py3-none-any.whl (112 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.6/112.6 kB\u001b[0m \u001b[31m189.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading xformers-0.0.29.post1-cp311-cp311-manylinux_2_28_x86_64.whl (15.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.3/15.3 MB\u001b[0m \u001b[31m84.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading torch-2.5.1-cp311-cp311-manylinux1_x86_64.whl (906.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m906.5/906.5 MB\u001b[0m \u001b[31m78.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m75.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m76.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m74.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m83.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m80.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m82.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m58.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m80.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m88.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m72.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m94.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m185.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m91.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading triton-3.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m90.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading docstring_parser-0.16-py3-none-any.whl (36 kB)\n",
      "Downloading pydantic-2.10.4-py3-none-any.whl (431 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m431.8/431.8 kB\u001b[0m \u001b[31m86.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pydantic_core-2.27.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m78.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tiktoken-0.7.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m82.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Building wheels for collected packages: fire\n",
      "  Building wheel for fire (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for fire: filename=fire-0.7.0-py3-none-any.whl size=114248 sha256=d635455bfeb3cbec68858ca384d95f9b5934e7242cd4de847494fd992a13ed95\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-wmm63494/wheels/46/54/24/1624fd5b8674eb1188623f7e8e17cdf7c0f6c24b609dfb8a89\n",
      "Successfully built fire\n",
      "Installing collected packages: sentencepiece, typing-extensions, triton, sympy, pillow, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, fire, docstring-parser, annotated-types, tiktoken, simple-parsing, pydantic-core, nvidia-cusparse-cu12, nvidia-cudnn-cu12, pydantic, nvidia-cusolver-cu12, torch, mistral_common, xformers, mistral_inference\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.9.0\n",
      "    Uninstalling typing_extensions-4.9.0:\n",
      "      Successfully uninstalled typing_extensions-4.9.0\n",
      "  Attempting uninstall: triton\n",
      "    Found existing installation: triton 2.1.0\n",
      "    Uninstalling triton-2.1.0:\n",
      "      Successfully uninstalled triton-2.1.0\n",
      "  Attempting uninstall: sympy\n",
      "    Found existing installation: sympy 1.12\n",
      "    Uninstalling sympy-1.12:\n",
      "      Successfully uninstalled sympy-1.12\n",
      "  Attempting uninstall: pillow\n",
      "    Found existing installation: pillow 10.2.0\n",
      "    Uninstalling pillow-10.2.0:\n",
      "      Successfully uninstalled pillow-10.2.0\n",
      "  Attempting uninstall: nvidia-nvtx-cu12\n",
      "    Found existing installation: nvidia-nvtx-cu12 12.1.105\n",
      "    Uninstalling nvidia-nvtx-cu12-12.1.105:\n",
      "      Successfully uninstalled nvidia-nvtx-cu12-12.1.105\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.3.101\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.3.101:\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.3.101\n",
      "  Attempting uninstall: nvidia-nccl-cu12\n",
      "    Found existing installation: nvidia-nccl-cu12 2.18.1\n",
      "    Uninstalling nvidia-nccl-cu12-2.18.1:\n",
      "      Successfully uninstalled nvidia-nccl-cu12-2.18.1\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.2.106\n",
      "    Uninstalling nvidia-curand-cu12-10.3.2.106:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.2.106\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.0.2.54\n",
      "    Uninstalling nvidia-cufft-cu12-11.0.2.54:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.0.2.54\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.1.105\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.1.105:\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.1.105\n",
      "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
      "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.1.105\n",
      "    Uninstalling nvidia-cuda-nvrtc-cu12-12.1.105:\n",
      "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.1.105\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.1.105\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.1.105:\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.1.105\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.1.3.1\n",
      "    Uninstalling nvidia-cublas-cu12-12.1.3.1:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.1.3.1\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.1.0.106\n",
      "    Uninstalling nvidia-cusparse-cu12-12.1.0.106:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.1.0.106\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 8.9.2.26\n",
      "    Uninstalling nvidia-cudnn-cu12-8.9.2.26:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-8.9.2.26\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.4.5.107\n",
      "    Uninstalling nvidia-cusolver-cu12-11.4.5.107:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.4.5.107\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.1.2\n",
      "    Uninstalling torch-2.1.2:\n",
      "      Successfully uninstalled torch-2.1.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchaudio 2.1.2 requires torch==2.1.2, but you have torch 2.5.1 which is incompatible.\n",
      "torchvision 0.16.2 requires torch==2.1.2, but you have torch 2.5.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed annotated-types-0.7.0 docstring-parser-0.16 fire-0.7.0 mistral_common-1.5.1 mistral_inference-1.5.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.4.127 pillow-10.4.0 pydantic-2.10.4 pydantic-core-2.27.2 sentencepiece-0.2.0 simple-parsing-0.1.6 sympy-1.13.1 tiktoken-0.7.0 torch-2.5.1 triton-3.1.0 typing-extensions-4.12.2 xformers-0.0.29.post1\n"
     ]
    }
   ],
   "source": [
    "!pip install mistral_inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82baa988-7514-4bd6-839c-d44ec315874e",
   "metadata": {},
   "source": [
    "# Model Information\n",
    "\n",
    "Source: https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3 \\\n",
    "Gated Model so it needs a prior login, access verification and usage of an access token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40220ecb-759f-475d-b453-28014427f5ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login(token=\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "683a5ea8-4e50-4473-b5b2-cb5e5a2edb5f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4790a7b92e348c6ac45abfe41280f8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'/home/jovyan/mistral_models/7B-Instruct-v0.3'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "from pathlib import Path\n",
    "\n",
    "mistral_models_path = Path.home().joinpath('mistral_models', '7B-Instruct-v0.3')\n",
    "mistral_models_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "snapshot_download(repo_id=\"mistralai/Mistral-7B-Instruct-v0.3\", allow_patterns=[\"params.json\", \"consolidated.safetensors\", \"tokenizer.model.v3\"], local_dir=mistral_models_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44ad785-10d9-4402-8591-8981e3585ee4",
   "metadata": {},
   "source": [
    "# Request Steps\n",
    "The basic flow of using the local LLM can be described as follows:\n",
    "1. Define the human-readable message as a string\n",
    "2. Encapsulate the message from 1. into a structured object the LLM can work with\n",
    "3. Encode the object from 2. into tokens (basically numbers) that can interpreted by the LLM\n",
    "4. Pass the tokens to the LLM with additional parameters and receive the answer in tokens\n",
    "5. Convert the generated tokens back into a human-readable message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73385354-a811-446c-bab2-5987333114d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from mistral_inference.transformer import Transformer\n",
    "from mistral_inference.generate import generate\n",
    "\n",
    "from mistral_common.tokens.tokenizers.mistral import MistralTokenizer\n",
    "from mistral_common.protocol.instruct.messages import UserMessage, AssistantMessage\n",
    "from mistral_common.protocol.instruct.request import ChatCompletionRequest\n",
    "\n",
    "# loads a \"dictionary\" of how to convert text into numbers and back\n",
    "# used by the LLM to understand text \n",
    "tokenizer = MistralTokenizer.from_file(f\"{mistral_models_path}/tokenizer.model.v3\")\n",
    "model = Transformer.from_folder(mistral_models_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e39aaa6a-eebf-4343-bfac-20f56df47dd3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No, 2003 is not a prime number. A prime number is a natural number greater than 1 that has no positive divisors other than 1 and itself. For 2003, it can be divided evenly by 1, 3, 667, and 2003, so it does not meet the criteria to be a prime number.\n"
     ]
    }
   ],
   "source": [
    "# 1.\n",
    "user_message = \"Is 2003 a prime number?\"\n",
    "\n",
    "# 2.\n",
    "completion_request = ChatCompletionRequest(messages=[UserMessage(content=user_message)])\n",
    "\n",
    "# 3.\n",
    "tokens = tokenizer.encode_chat_completion(completion_request).tokens\n",
    "\n",
    "# 4.\n",
    "out_tokens, _ = generate([tokens], model, max_tokens=256, temperature=0.0, eos_id=tokenizer.instruct_tokenizer.tokenizer.eos_id)  # end of sequence ID\n",
    "# Temperature:\n",
    "#  -> 0.0 deterministic, always same/similar answer\n",
    "#  -> 1.0 more creative, varying responses\n",
    "\n",
    "# 5.\n",
    "result = tokenizer.instruct_tokenizer.tokenizer.decode(out_tokens[0])\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1062be22-4b35-4646-9e81-0f78168aa6e9",
   "metadata": {},
   "source": [
    "# Chat History\n",
    "To simulate a conversation, meaning a back and forth of different messages between the user and the LLM, we need to save the chat history.\\\n",
    "Then, with each LLM call (`generate(...)`) we pass the previous history as an array of UserMessage and AssistantMessage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "645c1f13-8299-41b9-b967-5b9358dbdf72",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I apologize for the mistake in my previous response. You are correct that 2003 is a prime number. I made an error in my calculation. A prime number is a natural number greater than 1 that has no positive divisors other than 1 and itself. Since 2003 cannot be divided evenly by any number between 2 and 2002, it is a prime number. I apologize for any confusion my previous response may have caused. Thank you for bringing this to my attention.\n"
     ]
    }
   ],
   "source": [
    "message_history=[\n",
    "    UserMessage(content=user_message),  # from prior question\n",
    "    AssistantMessage(content=result),  # from prior question\n",
    "    UserMessage(content=\"Are you sure?\")\n",
    "]\n",
    "\n",
    "completion_request = ChatCompletionRequest(messages=message_history)\n",
    "\n",
    "tokens = tokenizer.encode_chat_completion(completion_request).tokens\n",
    "\n",
    "out_tokens, _ = generate([tokens], model, max_tokens=256, temperature=0.0, eos_id=tokenizer.instruct_tokenizer.tokenizer.eos_id)\n",
    "\n",
    "result = tokenizer.instruct_tokenizer.tokenizer.decode(out_tokens[0])\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b4bdfe-8424-41bf-927a-ed7ad1a90dc1",
   "metadata": {},
   "source": [
    "# Memory Usage Information\n",
    "Open Terminal\n",
    "- `nvidia-smi` to check current status of GPUs\n",
    "- `PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True` to make room for the model to run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83964b10-8ae6-4ed3-9388-ad1aade148d3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available GPU memory: 50.82GB\n",
      "Reserved GPU memory: 0.00GB\n",
      "Allocated GPU memory: 0.00GB\n",
      "Available GPU memory: 50.82GB\n",
      "Reserved GPU memory: 0.00GB\n",
      "Allocated GPU memory: 0.00GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(f\"Available GPU memory: {torch.cuda.get_device_properties(0).total_memory/1e9:.2f}GB\")\n",
    "print(f\"Reserved GPU memory: {torch.cuda.memory_reserved(0)/1e9:.2f}GB\")\n",
    "print(f\"Allocated GPU memory: {torch.cuda.memory_allocated(0)/1e9:.2f}GB\")\n",
    "\n",
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "print(f\"Available GPU memory: {torch.cuda.get_device_properties(0).total_memory/1e9:.2f}GB\")\n",
    "print(f\"Reserved GPU memory: {torch.cuda.memory_reserved(0)/1e9:.2f}GB\")\n",
    "print(f\"Allocated GPU memory: {torch.cuda.memory_allocated(0)/1e9:.2f}GB\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
