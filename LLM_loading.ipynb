{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9d119a8-5c71-4244-ba97-9e10463fd17e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting mistral_inference\n",
      "  Downloading mistral_inference-1.5.0-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting fire>=0.6.0 (from mistral_inference)\n",
      "  Downloading fire-0.7.0.tar.gz (87 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: mistral_common>=1.4.0 in /opt/conda/lib/python3.11/site-packages (from mistral_inference) (1.5.1)\n",
      "Requirement already satisfied: pillow>=10.3.0 in /opt/conda/lib/python3.11/site-packages (from mistral_inference) (10.4.0)\n",
      "Requirement already satisfied: safetensors>=0.4.0 in /opt/conda/lib/python3.11/site-packages (from mistral_inference) (0.4.5)\n",
      "Collecting simple-parsing>=0.1.5 (from mistral_inference)\n",
      "  Downloading simple_parsing-0.1.6-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: xformers>=0.0.24 in /opt/conda/lib/python3.11/site-packages (from mistral_inference) (0.0.28.post3)\n",
      "Requirement already satisfied: termcolor in /opt/conda/lib/python3.11/site-packages (from fire>=0.6.0->mistral_inference) (2.4.0)\n",
      "Requirement already satisfied: jsonschema<5.0.0,>=4.21.1 in /opt/conda/lib/python3.11/site-packages (from mistral_common>=1.4.0->mistral_inference) (4.21.1)\n",
      "Requirement already satisfied: numpy>=1.25 in /opt/conda/lib/python3.11/site-packages (from mistral_common>=1.4.0->mistral_inference) (1.26.3)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.6.1 in /opt/conda/lib/python3.11/site-packages (from mistral_common>=1.4.0->mistral_inference) (2.10.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /opt/conda/lib/python3.11/site-packages (from mistral_common>=1.4.0->mistral_inference) (2.31.0)\n",
      "Requirement already satisfied: sentencepiece==0.2.0 in /opt/conda/lib/python3.11/site-packages (from mistral_common>=1.4.0->mistral_inference) (0.2.0)\n",
      "Requirement already satisfied: tiktoken<0.8.0,>=0.7.0 in /opt/conda/lib/python3.11/site-packages (from mistral_common>=1.4.0->mistral_inference) (0.7.0)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.11.0 in /opt/conda/lib/python3.11/site-packages (from mistral_common>=1.4.0->mistral_inference) (4.12.2)\n",
      "Collecting docstring-parser<1.0,>=0.15 (from simple-parsing>=0.1.5->mistral_inference)\n",
      "  Downloading docstring_parser-0.16-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: torch==2.5.1 in /opt/conda/lib/python3.11/site-packages (from xformers>=0.0.24->mistral_inference) (2.5.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from torch==2.5.1->xformers>=0.0.24->mistral_inference) (3.16.1)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch==2.5.1->xformers>=0.0.24->mistral_inference) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch==2.5.1->xformers>=0.0.24->mistral_inference) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.11/site-packages (from torch==2.5.1->xformers>=0.0.24->mistral_inference) (2023.12.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /opt/conda/lib/python3.11/site-packages (from torch==2.5.1->xformers>=0.0.24->mistral_inference) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /opt/conda/lib/python3.11/site-packages (from torch==2.5.1->xformers>=0.0.24->mistral_inference) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /opt/conda/lib/python3.11/site-packages (from torch==2.5.1->xformers>=0.0.24->mistral_inference) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /opt/conda/lib/python3.11/site-packages (from torch==2.5.1->xformers>=0.0.24->mistral_inference) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /opt/conda/lib/python3.11/site-packages (from torch==2.5.1->xformers>=0.0.24->mistral_inference) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /opt/conda/lib/python3.11/site-packages (from torch==2.5.1->xformers>=0.0.24->mistral_inference) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /opt/conda/lib/python3.11/site-packages (from torch==2.5.1->xformers>=0.0.24->mistral_inference) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /opt/conda/lib/python3.11/site-packages (from torch==2.5.1->xformers>=0.0.24->mistral_inference) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /opt/conda/lib/python3.11/site-packages (from torch==2.5.1->xformers>=0.0.24->mistral_inference) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /opt/conda/lib/python3.11/site-packages (from torch==2.5.1->xformers>=0.0.24->mistral_inference) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /opt/conda/lib/python3.11/site-packages (from torch==2.5.1->xformers>=0.0.24->mistral_inference) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /opt/conda/lib/python3.11/site-packages (from torch==2.5.1->xformers>=0.0.24->mistral_inference) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /opt/conda/lib/python3.11/site-packages (from torch==2.5.1->xformers>=0.0.24->mistral_inference) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/conda/lib/python3.11/site-packages (from torch==2.5.1->xformers>=0.0.24->mistral_inference) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from sympy==1.13.1->torch==2.5.1->xformers>=0.0.24->mistral_inference) (1.3.0)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /opt/conda/lib/python3.11/site-packages (from jsonschema<5.0.0,>=4.21.1->mistral_common>=1.4.0->mistral_inference) (23.2.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.11/site-packages (from jsonschema<5.0.0,>=4.21.1->mistral_common>=1.4.0->mistral_inference) (2023.12.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /opt/conda/lib/python3.11/site-packages (from jsonschema<5.0.0,>=4.21.1->mistral_common>=1.4.0->mistral_inference) (0.32.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.11/site-packages (from jsonschema<5.0.0,>=4.21.1->mistral_common>=1.4.0->mistral_inference) (0.17.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.6.1->mistral_common>=1.4.0->mistral_inference) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /opt/conda/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.6.1->mistral_common>=1.4.0->mistral_inference) (2.27.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests<3.0.0,>=2.0.0->mistral_common>=1.4.0->mistral_inference) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests<3.0.0,>=2.0.0->mistral_common>=1.4.0->mistral_inference) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests<3.0.0,>=2.0.0->mistral_common>=1.4.0->mistral_inference) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests<3.0.0,>=2.0.0->mistral_common>=1.4.0->mistral_inference) (2023.11.17)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /opt/conda/lib/python3.11/site-packages (from tiktoken<0.8.0,>=0.7.0->mistral_common>=1.4.0->mistral_inference) (2024.11.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch==2.5.1->xformers>=0.0.24->mistral_inference) (2.1.4)\n",
      "Downloading mistral_inference-1.5.0-py3-none-any.whl (30 kB)\n",
      "Downloading simple_parsing-0.1.6-py3-none-any.whl (112 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.6/112.6 kB\u001b[0m \u001b[31m201.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading docstring_parser-0.16-py3-none-any.whl (36 kB)\n",
      "Building wheels for collected packages: fire\n",
      "  Building wheel for fire (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for fire: filename=fire-0.7.0-py3-none-any.whl size=114248 sha256=ea79363b97f6c2a08184408fcc350cd3614d67c5ba06ca799a0411755aef47fa\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-b6n3rvtu/wheels/46/54/24/1624fd5b8674eb1188623f7e8e17cdf7c0f6c24b609dfb8a89\n",
      "Successfully built fire\n",
      "Installing collected packages: fire, docstring-parser, simple-parsing, mistral_inference\n",
      "Successfully installed docstring-parser-0.16 fire-0.7.0 mistral_inference-1.5.0 simple-parsing-0.1.6\n"
     ]
    }
   ],
   "source": [
    "!pip install mistral_inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82baa988-7514-4bd6-839c-d44ec315874e",
   "metadata": {},
   "source": [
    "# Model Information\n",
    "\n",
    "Source: https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3 \\\n",
    "Gated Model so it needs a prior login, access verification and usage of an access token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40220ecb-759f-475d-b453-28014427f5ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login(token=\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "683a5ea8-4e50-4473-b5b2-cb5e5a2edb5f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6c28ad7cded4c5fb70678e7176bb0d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ca87b72ae53404d85f34f0d2700a06c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "params.json:   0%|          | 0.00/202 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d06a8202b6148a88f8596c3e1cb77f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model.v3:   0%|          | 0.00/587k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbef21f4ab3d4e918b965381b9de3e78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "consolidated.safetensors:   0%|          | 0.00/14.5G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'/home/jovyan/mistral_models/7B-Instruct-v0.3'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "from pathlib import Path\n",
    "\n",
    "mistral_models_path = Path.home().joinpath('mistral_models', '7B-Instruct-v0.3')\n",
    "mistral_models_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "snapshot_download(repo_id=\"mistralai/Mistral-7B-Instruct-v0.3\", allow_patterns=[\"params.json\", \"consolidated.safetensors\", \"tokenizer.model.v3\"], local_dir=mistral_models_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44ad785-10d9-4402-8591-8981e3585ee4",
   "metadata": {},
   "source": [
    "# Request Steps\n",
    "The basic flow of using the local LLM can be described as follows:\n",
    "1. Define the human-readable message as a string\n",
    "2. Encapsulate the message from 1. into a structured object the LLM can work with\n",
    "3. Encode the object from 2. into tokens (basically numbers) that can interpreted by the LLM\n",
    "4. Pass the tokens to the LLM with additional parameters and receive the answer in tokens\n",
    "5. Convert the generated tokens back into a human-readable message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "73385354-a811-446c-bab2-5987333114d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from mistral_inference.transformer import Transformer\n",
    "from mistral_inference.generate import generate\n",
    "\n",
    "from mistral_common.tokens.tokenizers.mistral import MistralTokenizer\n",
    "from mistral_common.protocol.instruct.messages import UserMessage, AssistantMessage\n",
    "from mistral_common.protocol.instruct.request import ChatCompletionRequest\n",
    "\n",
    "# loads a \"dictionary\" of how to convert text into numbers and back\n",
    "# used by the LLM to understand text \n",
    "tokenizer = MistralTokenizer.from_file(f\"{mistral_models_path}/tokenizer.model.v3\")\n",
    "model = Transformer.from_folder(mistral_models_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e39aaa6a-eebf-4343-bfac-20f56df47dd3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No, 2003 is not a prime number. A prime number is a natural number greater than 1 that has no positive divisors other than 1 and itself. For 2003, it can be divided evenly by 1, 3, 667, and 2003, so it does not meet the criteria to be a prime number.\n"
     ]
    }
   ],
   "source": [
    "# 1.\n",
    "user_message = \"Is 2003 a prime number?\"\n",
    "\n",
    "# 2.\n",
    "completion_request = ChatCompletionRequest(messages=[UserMessage(content=user_message)])\n",
    "\n",
    "# 3.\n",
    "tokens = tokenizer.encode_chat_completion(completion_request).tokens\n",
    "\n",
    "# 4.\n",
    "out_tokens, _ = generate([tokens], model, max_tokens=256, temperature=0.0, eos_id=tokenizer.instruct_tokenizer.tokenizer.eos_id)  # end of sequence ID\n",
    "# Temperature:\n",
    "#  -> 0.0 deterministic, always same/similar answer\n",
    "#  -> 1.0 more creative, varying responses\n",
    "\n",
    "# 5.\n",
    "result = tokenizer.instruct_tokenizer.tokenizer.decode(out_tokens[0])\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1062be22-4b35-4646-9e81-0f78168aa6e9",
   "metadata": {},
   "source": [
    "# Chat History\n",
    "To simulate a conversation, meaning a back and forth of different messages between the user and the LLM, we need to save the chat history.\\\n",
    "Then, with each LLM call (`generate(...)`) we pass the previous history as an array of UserMessage and AssistantMessage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "645c1f13-8299-41b9-b967-5b9358dbdf72",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I apologize for the mistake in my previous response. You are correct that 2003 is a prime number. I made an error in my calculation. A prime number is a natural number greater than 1 that has no positive divisors other than 1 and itself. Since 2003 cannot be divided evenly by any number between 2 and 2002, it is a prime number. I apologize for any confusion my previous response may have caused. Thank you for bringing this to my attention.\n"
     ]
    }
   ],
   "source": [
    "message_history=[\n",
    "    UserMessage(content=user_message),  # from prior question\n",
    "    AssistantMessage(content=result),  # from prior question\n",
    "    UserMessage(content=\"Are you sure?\")\n",
    "]\n",
    "\n",
    "completion_request = ChatCompletionRequest(messages=message_history)\n",
    "\n",
    "tokens = tokenizer.encode_chat_completion(completion_request).tokens\n",
    "\n",
    "out_tokens, _ = generate([tokens], model, max_tokens=256, temperature=0.0, eos_id=tokenizer.instruct_tokenizer.tokenizer.eos_id)\n",
    "\n",
    "result = tokenizer.instruct_tokenizer.tokenizer.decode(out_tokens[0])\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b4bdfe-8424-41bf-927a-ed7ad1a90dc1",
   "metadata": {},
   "source": [
    "# Memory Usage Information\n",
    "Open Terminal\n",
    "- `nvidia-smi` to check current status of GPUs\n",
    "- `PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True` to make room for the model to run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "83964b10-8ae6-4ed3-9388-ad1aade148d3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available GPU memory: 50.82GB\n",
      "Reserved GPU memory: 1.91GB\n",
      "Allocated GPU memory: 1.90GB\n",
      "Available GPU memory: 50.82GB\n",
      "Reserved GPU memory: 1.91GB\n",
      "Allocated GPU memory: 0.00GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(f\"Available GPU memory: {torch.cuda.get_device_properties(0).total_memory/1e9:.2f}GB\")\n",
    "print(f\"Reserved GPU memory: {torch.cuda.memory_reserved(0)/1e9:.2f}GB\")\n",
    "print(f\"Allocated GPU memory: {torch.cuda.memory_allocated(0)/1e9:.2f}GB\")\n",
    "\n",
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "print(f\"Available GPU memory: {torch.cuda.get_device_properties(0).total_memory/1e9:.2f}GB\")\n",
    "print(f\"Reserved GPU memory: {torch.cuda.memory_reserved(0)/1e9:.2f}GB\")\n",
    "print(f\"Allocated GPU memory: {torch.cuda.memory_allocated(0)/1e9:.2f}GB\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
